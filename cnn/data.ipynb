{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "\n",
    "    def __init__(self, valid_ratio, batch_size):\n",
    "        \n",
    "        \"\"\"Downloads, splits and normalizes the data. Provides dataloaders for training\"\"\"\n",
    "        \n",
    "        mean_std = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        \n",
    "        transforms = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(*mean_std)])\n",
    "\n",
    "#         train_data = datasets.CIFAR10(root=\"../data\",\n",
    "#                             train=True,\n",
    "#                             download=True)\n",
    "\n",
    "#         train_data.data = torch.tensor(train_data.data)\n",
    "\n",
    "#         channels = train_data.data.split(1, dim=-1)\n",
    "#         channel_tensors = [channel.squeeze(-1) for channel in channels]\n",
    "\n",
    "#         means = [z.float().mean() / 255 for z in channel_tensors]\n",
    "#         stds = [z.float().std() / 255 for z in channel_tensors]\n",
    "\n",
    "#         train_transforms = transforms.Compose([\n",
    "#                             transforms.ToTensor(),\n",
    "#                             transforms.Normalize(mean=means, std=stds)\n",
    "#                                       ])\n",
    "\n",
    "#         test_transforms = transforms.Compose([\n",
    "#                                 transforms.ToTensor(),\n",
    "#                                 transforms.Normalize(mean=means, std=stds)\n",
    "#                                             ])\n",
    "\n",
    "        train_data = datasets.CIFAR10(root=\"../data\",\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transforms)\n",
    "\n",
    "        test_data = datasets.CIFAR10(root=\"../data\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transforms)\n",
    "        \n",
    "        n_train_examples = int(len(train_data) * valid_ratio)\n",
    "        n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "        train_data, valid_data = data.random_split(train_data,\n",
    "                                                [n_train_examples, n_valid_examples])\n",
    "\n",
    "        valid_data = copy.deepcopy(valid_data)\n",
    "        valid_data.dataset.transform = test_transforms\n",
    "\n",
    "        self.train_loader = data.DataLoader(train_data,\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=batch_size, num_workers=2)\n",
    "\n",
    "        self.valid_loader = data.DataLoader(valid_data,\n",
    "                                        batch_size=batch_size, num_workers=2)\n",
    "        \n",
    "        self.test_loader = data.DataLoader(test_data, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "\n",
    "    #     # downloads the data\n",
    "    #     self.train_data = datasets.CIFAR10(root=ROOT, train=True, download=True)\n",
    "    #     self.test_data = datasets.CIFAR10(root=ROOT, train=False,download=True)\n",
    "\n",
    "    #     # splitting the data\n",
    "    #     n_train_examples = int(len(self.train_data) * valid_ratio)\n",
    "    #     n_valid_examples = len(self.train_data) - n_train_examples\n",
    "    #     self.train_data, self.valid_data = data.random_split(self.train_data, [n_train_examples, n_valid_examples])\n",
    "    #     self.valid_data = copy.deepcopy(self.valid_data)\n",
    "\n",
    "    #     # normalizing the data\n",
    "\n",
    "    #     t_mean, t_std = self.calculate_mean_std(self.train_data)\n",
    "    #     train_transforms = transforms.Compose([\n",
    "    #                         transforms.ToTensor(),\n",
    "    #                         transforms.Normalize(mean=t_mean, std=t_std)])\n",
    "    #     v_mean, v_std = self.calculate_mean_std(self.valid_data)\n",
    "    #     valid_transforms = transforms.Compose([\n",
    "    #                         transforms.ToTensor(),\n",
    "    #                         transforms.Normalize(mean=v_mean, std=v_std)])\n",
    "    #     te_mean, te_std = self.calculate_mean_std(self.test_data)\n",
    "    #     test_transforms = transforms.Compose([\n",
    "    #                         transforms.ToTensor(),\n",
    "    #                         transforms.Normalize(mean=te_mean, std=te_std)])\n",
    "        \n",
    "    #     self.train_data.transform = train_transforms\n",
    "    #     self.valid_data.transform = valid_transforms\n",
    "    #     self.test_data.transform = test_transforms\n",
    "\n",
    "    #     self.train_loader = data.DataLoader(self.train_data,\n",
    "    #                              shuffle=True,\n",
    "    #                              batch_size=batch_size, num_workers=2)\n",
    "    #     self.valid_loader = data.DataLoader(self.valid_data,\n",
    "    #                              batch_size=batch_size, num_workers=2)\n",
    "        \n",
    "    \n",
    "    # def calculate_mean_std(self, dataset):\n",
    "    #     \"\"\"Calculates the mean and std of a dataset\"\"\"\n",
    "    #     dataset.data = torch.tensor(dataset.data)\n",
    "    #     channels = dataset.data.split(1, dim=-1)\n",
    "    #     channel_tensors = [channel.squeeze(-1) for channel in channels]\n",
    "    #     means = [z.float().mean() / 255 for z in channel_tensors]\n",
    "    #     stds = [z.float().std() / 255 for z in channel_tensors]\n",
    "    #     return means, stds\n",
    "\n",
    "    class Inception(nn.Module):\n",
    "    \n",
    "    # c1--c5 are the number of output channels for each branch\n",
    "    def __init__(self, c1, c2, c3, c4, c5, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # branch 1\n",
    "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
    "        # branch 2\n",
    "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
    "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
    "        # branch 3\n",
    "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
    "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
    "        # branch 4 (added this branch to make the network wider)\n",
    "        self.b4_1 = nn.LazyConv2d(c4[0], kernel_size=1)\n",
    "        self.b4_2 = nn.LazyConv2d(c4[1], kernel_size=7, padding=3)\n",
    "        # branch 5\n",
    "        self.b5_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.b5_2 = nn.LazyConv2d(c5, kernel_size=1)\n",
    "    \n",
    "    # puts everything together\n",
    "    def forward(self, x):\n",
    "        b1 = F.relu(self.b1_1(x))\n",
    "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
    "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
    "        b4 = F.relu(self.b4_2(F.relu(self.b4_1(x))))\n",
    "        b5 = F.relu(self.b5_2(self.b5_1(x)))\n",
    "        return torch.cat((b1, b2, b3, b4, b5), dim=1)\n",
    "    \n",
    "    # inception block to see if we can weave in inception blocks into resnet for better results\n",
    "    def inc(self):\n",
    "        return nn.Sequential(Inception(32, (16, 32), (48, 64), (16, 32), 32),\n",
    "                         Inception(64, (32, 64), (96, 128), (32, 64), 64),\n",
    "                         nn.MaxPool2d(kernel_size=3, padding=1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
