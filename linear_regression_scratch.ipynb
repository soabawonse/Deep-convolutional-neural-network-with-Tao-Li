{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from pytorch.d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Linear Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the class Module stores the learnable parameters, so we initialize the weights and bias there.\n",
    "class LinearRegressionScratch(d2l.Module):\n",
    "    \"\"\"The linear regression model implemented from scratch\"\"\"\n",
    "    # the weights are intiialized by drawing random numbers from a normal distribution with mean 0 and sigma 0.01\n",
    "    # the number of inputs, the learning rate, and the standard deviation for the normal distribution\n",
    "    # this has only one feature\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # mean, std, shape, requires gradient or not\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward returns the value of the prediction y given the current parameters. in this case it is just a matrix multiplication added to a broadcasted scalar\n",
    "@d2l.add_to_class(LinearRegressionScratch)\n",
    "def forward(self, X):\n",
    "    return torch.matmul(X, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the loss based on the prediction and the actual label\n",
    "@d2l.add_to_class(LinearRegressionScratch)\n",
    "def loss(self, y_hat, y):\n",
    "    l = (y_hat - y) ** 2 / 2\n",
    "    return l.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the optimization algorithm, the stochastic gradient descent\n",
    "# at each step, we use a minibatch randomly drawn from the dataset, estimate the gradient of the loss with respect to the parameters, and then udpate the parameters in the direction that reduces the loss\n",
    "# this class is based on the template of the class DataModule\n",
    "class SGD(d2l.HyperParameters):\n",
    "    \"\"\"Minibatch stochastic gradient descent\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    # params is [weights, bias], and param.grad calculates their gradient\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "    \n",
    "    # sets all the gradients to 0, which must be run before a backpropagation step\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the configure_optimizers method in Module, which just returns an instance of the SGD class\n",
    "@d2l.add_to_class(LinearRegressionScratch)\n",
    "def configure_optimizers(self):\n",
    "    return SGD([self.w, self.b], self.lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the main training loop\n",
    "1. in each epoch, we iterate through the entire training dataset, passing once through every example\n",
    "2. in each iteration, we grab a minibatch of training examples and compute the loss through the model's training step method\n",
    "3. we then compute the gradients with respect to each aprameter\n",
    "4. finally we call the optimization algorithm to update the model parameters\n",
    "```\n",
    "initialize parameters (w, b)\n",
    "repeat until done\n",
    "    compute gradient with parameters (w, b)\n",
    "    update parameters (w, b)\n",
    "```\n",
    "We also pass the validation dataloader once in each epoch to measure the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)\n",
    "def prepare_batch(self, batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)\n",
    "# for each epoch\n",
    "def fit_epoch(self):\n",
    "    self.model.train()\n",
    "    # iterating over each minibatch\n",
    "    for batch in self.train_dataloader:\n",
    "        loss = self.model.training_step(self.prepare_batch(batch))\n",
    "        self.optim.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "            if self.gradient_clip_val > 0:\n",
    "                self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "            self.optim.step()\n",
    "        if self.val_dataloader is None:\n",
    "            return\n",
    "        self.model.eval()\n",
    "        for batch in self.val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                self.model.validation_step(self.prepare_batch(batch))\n",
    "            self.val_batch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39mSyntheticRegressionData(w\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor([\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m3.4\u001b[39m]), b\u001b[39m=\u001b[39m\u001b[39m4.2\u001b[39m)\n\u001b[1;32m      4\u001b[0m trainer \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data)\n",
      "File \u001b[0;32m~/d2l-en/pytorch/d2l/torch.py:286\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_batch_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[0;32m--> 286\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_epoch()\n",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m, in \u001b[0;36mfit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# iterating over each minibatch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader:\n\u001b[0;32m----> 7\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_batch(batch))\n\u001b[1;32m      8\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/d2l-en/pytorch/d2l/torch.py:215\u001b[0m, in \u001b[0;36mModule.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    214\u001b[0m     l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(\u001b[39mself\u001b[39m(\u001b[39m*\u001b[39mbatch[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), batch[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplot(\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m, l, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m l\n",
      "File \u001b[0;32m~/d2l-en/pytorch/d2l/torch.py:209\u001b[0m, in \u001b[0;36mModule.plot\u001b[0;34m(self, key, value, train)\u001b[0m\n\u001b[1;32m    206\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    207\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mnum_val_batches \u001b[39m/\u001b[39m \\\n\u001b[1;32m    208\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_valid_per_epoch\n\u001b[0;32m--> 209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard\u001b[39m.\u001b[39mdraw(x, d2l\u001b[39m.\u001b[39mnumpy(d2l\u001b[39m.\u001b[39;49mto(value, d2l\u001b[39m.\u001b[39;49mcpu())),\n\u001b[1;32m    210\u001b[0m                 (\u001b[39m'\u001b[39m\u001b[39mtrain_\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m train \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m key,\n\u001b[1;32m    211\u001b[0m                 every_n\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(n))\n",
      "File \u001b[0;32m~/d2l-en/pytorch/d2l/torch.py:3506\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3504\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: x\u001b[39m.\u001b[39mnumel(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   3505\u001b[0m reshape \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: x\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3506\u001b[0m to \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: x\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   3507\u001b[0m reduce_sum \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: x\u001b[39m.\u001b[39msum(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   3508\u001b[0m argmax \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: x\u001b[39m.\u001b[39margmax(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# now to generate some artificial data using SyntheticRegressionData\n",
    "# think there is a type somewhere in my code\n",
    "model = LinearRegressionScratch(2, lr=0.03)\n",
    "data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
