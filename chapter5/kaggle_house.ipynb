{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleHouse(d2l.DataModule):\n",
    "    def __init__(self, batch_size, train=None, val=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if self.train is None:\n",
    "            self.raw_train = pd.read_csv(d2l.download(\n",
    "                d2l.DATA_URL + 'kaggle_house_pred_train.csv', self.root,\n",
    "                sha1_hash='585e9cc93e70b39160e7921475f9bcd7d31219ce'))\n",
    "            self.raw_val = pd.read_csv(d2l.download(\n",
    "                d2l.DATA_URL + 'kaggle_house_pred_test.csv', self.root,\n",
    "                sha1_hash='fa19780a7b011d9b009e8bff8e99922a8ee2eb90'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "(1459, 80)\n"
     ]
    }
   ],
   "source": [
    "data = KaggleHouse(batch_size=64)\n",
    "print(data.raw_train.shape)\n",
    "print(data.raw_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of data\n",
    "# we want to remove the first column named id, replace missing numerical values with the mean of that column, then we standardize the data by rescaling to zero mean and unit variance\n",
    "# changing the discrete data into one-hot encoding\n",
    "@d2l.add_to_class(KaggleHouse)\n",
    "def preprocess(self):\n",
    "    # remove the id and label columns\n",
    "    label = 'SalePrice'\n",
    "    features = pd.concat((self.raw_train.drop(columns=['Id', label]), self.raw_val.drop(columns=['Id'])))\n",
    "    # standardize numerical columns \n",
    "    numeric_features = features.dtypes[features.dtypes!='object'].index\n",
    "    features[numeric_features] = features[numeric_features].apply(lambda x: (x - x.mean() / x.std()))\n",
    "    # replace NaN numerical features by 0\n",
    "    features[numeric_features] = features[numeric_features].fillna(0)\n",
    "    # replace discrete features by one-hot encoding\n",
    "    features = pd.get_dummies(features, dummy_na=True)\n",
    "    # save preprocessed features\n",
    "    self.train = features[:self.raw_train.shape[0]].copy()\n",
    "    self.train[label] = self.raw_train[label]\n",
    "    self.val = features[self.raw_train.shape[0]:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 331)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.preprocess()\n",
    "data.train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(KaggleHouse)\n",
    "def get_dataloader(self, train):\n",
    "    label = 'SalePrice'\n",
    "    data = self.train if train else self.val\n",
    "    if label not in data: return\n",
    "    get_tensor = lambda x: torch.tensor(x.values, dtype=torch.float32)\n",
    "    # Logarithm of prices\n",
    "    tensors = (get_tensor(data.drop(columns=[label])),  # X\n",
    "               torch.log(get_tensor(data[label])).reshape((-1, 1)))  # Y\n",
    "    return self.get_tensorloader(tensors, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use k-fold cross validation to select the model\n",
    "# this iterates over the data k times, each time selecting a different portion for the validation set and the rest as training set, and adding each of of the KaggleHouse model objects into rets\n",
    "def k_fold_data(data, k):\n",
    "    rets = []\n",
    "    fold_size = data.train.shape[0] // k\n",
    "    for j in range(k):\n",
    "        idx = range(j * fold_size, (j+1) * fold_size)\n",
    "        rets.append(KaggleHouse(data.batch_size, data.train.drop(index=idx),\n",
    "                                data.train.loc[idx]))\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(trainer, data, k, lr):\n",
    "    val_loss, models = [], []\n",
    "    for i, data_fold in enumerate(k_fold_data(data, k)):\n",
    "        model = d2l.LinearRegression(lr)\n",
    "        model.board.yscale='log'\n",
    "        if i != 0: model.board.display = False\n",
    "        trainer.fit(model, data_fold)\n",
    "        val_loss.append(float(model.board.data['val_loss'][-1].y))\n",
    "        models.append(model)\n",
    "    print(f'average validation log mse = {sum(val_loss)/len(val_loss)}')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisali/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m models \u001b[39m=\u001b[39m k_fold(trainer, data, k\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m, in \u001b[0;36mk_fold\u001b[0;34m(trainer, data, k, lr)\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mboard\u001b[39m.\u001b[39myscale\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m: model\u001b[39m.\u001b[39mboard\u001b[39m.\u001b[39mdisplay \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data_fold)\n\u001b[1;32m      8\u001b[0m val_loss\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(model\u001b[39m.\u001b[39mboard\u001b[39m.\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39my))\n\u001b[1;32m      9\u001b[0m models\u001b[39m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/d2l/torch.py:267\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, model, data):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_data(data)\n\u001b[1;32m    268\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_model(model)\n\u001b[1;32m    269\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfigure_optimizers()\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/d2l/torch.py:255\u001b[0m, in \u001b[0;36mTrainer.prepare_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_data\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m--> 255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mtrain_dataloader()\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_dataloader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mval_dataloader()\n\u001b[1;32m    257\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_train_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/d2l/torch.py:236\u001b[0m, in \u001b[0;36mDataModule.train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_dataloader\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_dataloader(train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m      6\u001b[0m get_tensor \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: torch\u001b[39m.\u001b[39mtensor(x\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Logarithm of prices\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tensors \u001b[39m=\u001b[39m (get_tensor(data\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m[label])),  \u001b[39m# X\u001b[39;00m\n\u001b[1;32m      9\u001b[0m            torch\u001b[39m.\u001b[39mlog(get_tensor(data[label]))\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)))  \u001b[39m# Y\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_tensorloader(tensors, train)\n",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m, in \u001b[0;36mget_dataloader.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39mif\u001b[39;00m train \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval\n\u001b[1;32m      5\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m data: \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m get_tensor \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: torch\u001b[39m.\u001b[39;49mtensor(x\u001b[39m.\u001b[39;49mvalues, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Logarithm of prices\u001b[39;00m\n\u001b[1;32m      8\u001b[0m tensors \u001b[39m=\u001b[39m (get_tensor(data\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[label])),  \u001b[39m# X\u001b[39;00m\n\u001b[1;32m      9\u001b[0m            torch\u001b[39m.\u001b[39mlog(get_tensor(data[label]))\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)))  \u001b[39m# Y\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "models = k_fold(trainer, data, k=5, lr=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
