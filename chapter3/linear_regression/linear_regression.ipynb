{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code does not exactly work, specifically, some function or gradient is wrong such that the loss is increasing instead of decreasing. However, I did understsand the architecture of the code better and did understand somewhat how to work with tensors and pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the data\n",
    "data = pd.read_csv('housing.csv')\n",
    "x = data.loc[:,[\"area\", \"bedrooms\", \"bathrooms\", \"stories\", \"parking\"]]\n",
    "x = x[:].values\n",
    "y = data.loc[:,\"price\"]\n",
    "y = y[:].values\n",
    "x_train = x[0:400]\n",
    "x_val = x[400:500]\n",
    "y_train = y[0:400]\n",
    "y_val = y[400:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7420    4    2    3    2]\n",
      " [8960    4    4    4    3]\n",
      " [9960    3    2    2    2]\n",
      " ...\n",
      " [5900    2    1    1    1]\n",
      " [3120    3    1    2    1]\n",
      " [7350    2    1    1    1]]\n",
      "[[ 3512     2     1     1     1]\n",
      " [ 9500     3     1     2     3]\n",
      " [ 5880     2     1     1     0]\n",
      " [12944     3     1     1     0]\n",
      " [ 4900     3     1     2     0]\n",
      " [ 3060     3     1     1     0]\n",
      " [ 5320     2     1     1     1]\n",
      " [ 2145     3     1     3     0]\n",
      " [ 4000     2     1     1     0]\n",
      " [ 3185     2     1     1     2]\n",
      " [ 3850     3     1     1     0]\n",
      " [ 2145     3     1     3     0]\n",
      " [ 2610     3     1     2     0]\n",
      " [ 1950     3     2     2     0]\n",
      " [ 4040     2     1     1     0]\n",
      " [ 4785     3     1     2     1]\n",
      " [ 3450     3     1     1     2]\n",
      " [ 3640     2     1     1     0]\n",
      " [ 3500     4     1     2     2]\n",
      " [ 4960     4     1     3     0]\n",
      " [ 4120     2     1     2     0]\n",
      " [ 4750     2     1     1     0]\n",
      " [ 3720     2     1     1     0]\n",
      " [ 3750     3     1     1     0]\n",
      " [ 3100     3     1     2     0]\n",
      " [ 3185     2     1     1     2]\n",
      " [ 2700     3     1     1     0]\n",
      " [ 2145     3     1     2     0]\n",
      " [ 4040     2     1     1     1]\n",
      " [ 4775     4     1     2     0]\n",
      " [ 2500     2     1     1     0]\n",
      " [ 3180     4     1     2     0]\n",
      " [ 6060     3     1     1     0]\n",
      " [ 3480     4     1     2     1]\n",
      " [ 3792     4     1     2     0]\n",
      " [ 4040     2     1     1     0]\n",
      " [ 2145     3     1     2     0]\n",
      " [ 5880     3     1     1     1]\n",
      " [ 4500     2     1     1     0]\n",
      " [ 3930     2     1     1     0]\n",
      " [ 3640     4     1     2     0]\n",
      " [ 4370     3     1     2     0]\n",
      " [ 2684     2     1     1     1]\n",
      " [ 4320     3     1     1     1]\n",
      " [ 3120     3     1     2     0]\n",
      " [ 3450     1     1     1     0]\n",
      " [ 3986     2     2     1     1]\n",
      " [ 3500     2     1     1     0]\n",
      " [ 4095     2     1     1     2]\n",
      " [ 1650     3     1     2     0]\n",
      " [ 3450     3     1     2     0]\n",
      " [ 6750     2     1     1     0]\n",
      " [ 9000     3     1     2     2]\n",
      " [ 3069     2     1     1     1]\n",
      " [ 4500     3     1     2     0]\n",
      " [ 5495     3     1     1     0]\n",
      " [ 2398     3     1     1     0]\n",
      " [ 3000     3     1     1     0]\n",
      " [ 3850     3     1     2     0]\n",
      " [ 3500     2     1     1     0]\n",
      " [ 8100     2     1     1     1]\n",
      " [ 4960     2     1     1     0]\n",
      " [ 2160     3     1     2     0]\n",
      " [ 3090     2     1     1     0]\n",
      " [ 4500     2     1     2     1]\n",
      " [ 3800     2     1     1     0]\n",
      " [ 3090     3     1     2     0]\n",
      " [ 3240     3     1     2     2]\n",
      " [ 2835     2     1     1     0]\n",
      " [ 4600     2     1     1     0]\n",
      " [ 5076     3     1     1     0]\n",
      " [ 3750     3     1     2     0]\n",
      " [ 3630     4     1     2     3]\n",
      " [ 8050     2     1     1     0]\n",
      " [ 4352     4     1     2     1]\n",
      " [ 3000     2     1     2     0]\n",
      " [ 5850     3     1     2     1]\n",
      " [ 4960     2     1     1     0]\n",
      " [ 3600     3     1     2     1]\n",
      " [ 3660     4     1     2     0]\n",
      " [ 3480     3     1     2     1]\n",
      " [ 2700     2     1     1     0]\n",
      " [ 3150     3     1     2     0]\n",
      " [ 6615     3     1     2     0]\n",
      " [ 3040     2     1     1     0]\n",
      " [ 3630     2     1     1     0]\n",
      " [ 6000     2     1     1     0]\n",
      " [ 5400     4     1     2     0]\n",
      " [ 5200     4     1     3     0]\n",
      " [ 3300     3     1     2     1]\n",
      " [ 4350     3     1     2     1]\n",
      " [ 2640     2     1     1     1]\n",
      " [ 2650     3     1     2     1]\n",
      " [ 3960     3     1     1     0]\n",
      " [ 6800     2     1     1     0]\n",
      " [ 4000     3     1     2     1]\n",
      " [ 4000     2     1     1     0]\n",
      " [ 3934     2     1     1     0]\n",
      " [ 2000     2     1     2     0]\n",
      " [ 3630     3     3     2     0]]\n",
      "[13300000 12250000 12250000 12215000 11410000 10850000 10150000 10150000\n",
      "  9870000  9800000  9800000  9681000  9310000  9240000  9240000  9100000\n",
      "  9100000  8960000  8890000  8855000  8750000  8680000  8645000  8645000\n",
      "  8575000  8540000  8463000  8400000  8400000  8400000  8400000  8400000\n",
      "  8295000  8190000  8120000  8080940  8043000  7980000  7962500  7910000\n",
      "  7875000  7840000  7700000  7700000  7560000  7560000  7525000  7490000\n",
      "  7455000  7420000  7420000  7420000  7350000  7350000  7350000  7350000\n",
      "  7343000  7245000  7210000  7210000  7140000  7070000  7070000  7035000\n",
      "  7000000  6930000  6930000  6895000  6860000  6790000  6790000  6755000\n",
      "  6720000  6685000  6650000  6650000  6650000  6650000  6650000  6650000\n",
      "  6629000  6615000  6615000  6580000  6510000  6510000  6510000  6475000\n",
      "  6475000  6440000  6440000  6419000  6405000  6300000  6300000  6300000\n",
      "  6300000  6300000  6293000  6265000  6230000  6230000  6195000  6195000\n",
      "  6195000  6160000  6160000  6125000  6107500  6090000  6090000  6090000\n",
      "  6083000  6083000  6020000  6020000  6020000  5950000  5950000  5950000\n",
      "  5950000  5950000  5950000  5950000  5950000  5943000  5880000  5880000\n",
      "  5873000  5873000  5866000  5810000  5810000  5810000  5803000  5775000\n",
      "  5740000  5740000  5740000  5740000  5740000  5652500  5600000  5600000\n",
      "  5600000  5600000  5600000  5600000  5600000  5600000  5600000  5565000\n",
      "  5565000  5530000  5530000  5530000  5523000  5495000  5495000  5460000\n",
      "  5460000  5460000  5460000  5425000  5390000  5383000  5320000  5285000\n",
      "  5250000  5250000  5250000  5250000  5250000  5250000  5250000  5250000\n",
      "  5250000  5243000  5229000  5215000  5215000  5215000  5145000  5145000\n",
      "  5110000  5110000  5110000  5110000  5075000  5040000  5040000  5040000\n",
      "  5040000  5033000  5005000  4970000  4970000  4956000  4935000  4907000\n",
      "  4900000  4900000  4900000  4900000  4900000  4900000  4900000  4900000\n",
      "  4900000  4900000  4900000  4900000  4893000  4893000  4865000  4830000\n",
      "  4830000  4830000  4830000  4795000  4795000  4767000  4760000  4760000\n",
      "  4760000  4753000  4690000  4690000  4690000  4690000  4690000  4690000\n",
      "  4655000  4620000  4620000  4620000  4620000  4620000  4613000  4585000\n",
      "  4585000  4550000  4550000  4550000  4550000  4550000  4550000  4550000\n",
      "  4543000  4543000  4515000  4515000  4515000  4515000  4480000  4480000\n",
      "  4480000  4480000  4480000  4473000  4473000  4473000  4445000  4410000\n",
      "  4410000  4403000  4403000  4403000  4382000  4375000  4340000  4340000\n",
      "  4340000  4340000  4340000  4319000  4305000  4305000  4277000  4270000\n",
      "  4270000  4270000  4270000  4270000  4270000  4235000  4235000  4200000\n",
      "  4200000  4200000  4200000  4200000  4200000  4200000  4200000  4200000\n",
      "  4200000  4200000  4200000  4200000  4200000  4200000  4200000  4200000\n",
      "  4193000  4193000  4165000  4165000  4165000  4130000  4130000  4123000\n",
      "  4098500  4095000  4095000  4095000  4060000  4060000  4060000  4060000\n",
      "  4060000  4025000  4025000  4025000  4007500  4007500  3990000  3990000\n",
      "  3990000  3990000  3990000  3920000  3920000  3920000  3920000  3920000\n",
      "  3920000  3920000  3885000  3885000  3850000  3850000  3850000  3850000\n",
      "  3850000  3850000  3850000  3836000  3815000  3780000  3780000  3780000\n",
      "  3780000  3780000  3780000  3773000  3773000  3773000  3745000  3710000\n",
      "  3710000  3710000  3710000  3710000  3703000  3703000  3675000  3675000\n",
      "  3675000  3675000  3640000  3640000  3640000  3640000  3640000  3640000\n",
      "  3640000  3640000  3640000  3633000  3605000  3605000  3570000  3570000\n",
      "  3570000  3570000  3535000  3500000  3500000  3500000  3500000  3500000\n",
      "  3500000  3500000  3500000  3500000  3500000  3500000  3500000  3500000]\n",
      "[3500000 3500000 3500000 3500000 3493000 3465000 3465000 3465000 3430000\n",
      " 3430000 3430000 3430000 3430000 3430000 3423000 3395000 3395000 3395000\n",
      " 3360000 3360000 3360000 3360000 3360000 3360000 3360000 3360000 3353000\n",
      " 3332000 3325000 3325000 3290000 3290000 3290000 3290000 3290000 3290000\n",
      " 3290000 3290000 3255000 3255000 3234000 3220000 3220000 3220000 3220000\n",
      " 3150000 3150000 3150000 3150000 3150000 3150000 3150000 3150000 3150000\n",
      " 3143000 3129000 3118850 3115000 3115000 3115000 3087000 3080000 3080000\n",
      " 3080000 3080000 3045000 3010000 3010000 3010000 3010000 3010000 3010000\n",
      " 3010000 3003000 2975000 2961000 2940000 2940000 2940000 2940000 2940000\n",
      " 2940000 2940000 2940000 2870000 2870000 2870000 2870000 2852500 2835000\n",
      " 2835000 2835000 2800000 2800000 2730000 2730000 2695000 2660000 2660000\n",
      " 2660000]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_val)\n",
    "print(y_train)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def __init__(self, num_features, lr):\n",
    "        \"\"\"Initializes the weights and biases of this model\"\"\"\n",
    "        # initializing w as a column vector, such that matrix operation Xw holds\n",
    "        self.w = torch.normal(0, 0.01, (num_features, 1), requires_grad=True, dtype=float)\n",
    "        # b only needs to be one, since for linear regression we want to end up with a single model with a single bias\n",
    "        self.b = torch.zeros(1, requires_grad=True, dtype=float)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Calculates the output of a minibatch of inputs\"\"\"\n",
    "        return (torch.matmul(X, self.w) + self.b)\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        \"\"\"Returns the loss value for a batch of prediction and actual results\"\"\"\n",
    "        l = (y_hat - y) ** 2 / 2\n",
    "        return l.mean()\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"Accepts a data batch and returns the loss value\"\"\"\n",
    "        # batch is an array of [X, y]\n",
    "        out = self.forward(batch[0])\n",
    "        l = self.loss(out, batch[1])\n",
    "        return l\n",
    "    \n",
    "    def update_parameters(self, batch):\n",
    "        \"\"\"Updates parameters a little bit towards the right direction\"\"\"\n",
    "        loss = self.training_step(batch)\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "            self.w -= self.lr * self.w.grad\n",
    "            self.b -= self.lr * self.b.grad\n",
    "            self.w.grad.zero_()\n",
    "            self.b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0107],\n",
      "        [ 0.0016],\n",
      "        [-0.0185],\n",
      "        [ 0.0149],\n",
      "        [-0.0238]], dtype=torch.float64, requires_grad=True)\n",
      "tensor([0.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = Module(5, 0.01)\n",
    "print(model.w)\n",
    "print(model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "\n",
    "    def __init__(self, batch_size, x_train, x_val, y_train, y_val):\n",
    "        self.batch_size = batch_size\n",
    "        self.x_train = x_train\n",
    "        self.x_val = x_val\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        iters = self.x_train.shape[0] // self.batch_size\n",
    "        for i in range(0, iters):\n",
    "            yield [torch.tensor(self.x_train[i:i+self.batch_size, :], requires_grad=True, dtype=float), torch.tensor(self.y_train[i:i+self.batch_size], requires_grad=True, dtype=float).reshape((self.batch_size, 1))]\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        iters = self.x_val.shape[0] // self.batch_size\n",
    "        for i in range(0, iters):\n",
    "            yield torch.tensor(self.x_val[i:i+self.batch_size, :], requires_grad=True, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 5)\n",
      "(100, 5)\n",
      "(400,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "data = DataModule(20, x_train, x_val, y_train, y_val)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 63681586415571.125\n",
      "Weights: tensor([[9.7592e+09],\n",
      "        [4.2623e+06],\n",
      "        [2.5886e+06],\n",
      "        [2.9134e+06],\n",
      "        [2.1885e+06]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([1122449.1059], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 4.552598657178302e+27\n",
      "Weights: tensor([[-9.3299e+16],\n",
      "        [-3.4398e+13],\n",
      "        [-2.0282e+13],\n",
      "        [-2.1999e+13],\n",
      "        [-1.6069e+13]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-9.0926e+12], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 3.9681313060915365e+41\n",
      "Weights: tensor([[8.5063e+23],\n",
      "        [3.1780e+20],\n",
      "        [1.7725e+20],\n",
      "        [1.8807e+20],\n",
      "        [1.3973e+20]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([8.4165e+19], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 3.094812267700947e+55\n",
      "Weights: tensor([[-7.2765e+30],\n",
      "        [-2.8662e+27],\n",
      "        [-1.5580e+27],\n",
      "        [-1.6567e+27],\n",
      "        [-1.1603e+27]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-7.3835e+26], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 2.1481653227818123e+69\n",
      "Weights: tensor([[5.9044e+37],\n",
      "        [2.3354e+34],\n",
      "        [1.2746e+34],\n",
      "        [1.3590e+34],\n",
      "        [8.7973e+33]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([6.0250e+33], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 1.4244661708244807e+83\n",
      "Weights: tensor([[-4.8251e+44],\n",
      "        [-1.8579e+41],\n",
      "        [-1.0825e+41],\n",
      "        [-1.1072e+41],\n",
      "        [-6.2622e+40]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-4.9113e+40], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 9.277350877280108e+96\n",
      "Weights: tensor([[3.8454e+51],\n",
      "        [1.5256e+48],\n",
      "        [8.0502e+47],\n",
      "        [9.2652e+47],\n",
      "        [4.9728e+47]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([3.9412e+47], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 5.670260015812959e+110\n",
      "Weights: tensor([[-2.9491e+58],\n",
      "        [-1.1854e+55],\n",
      "        [-5.9335e+54],\n",
      "        [-6.5719e+54],\n",
      "        [-3.5570e+54]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-3.0648e+54], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 2.5078754085556784e+124\n",
      "Weights: tensor([[1.7008e+65],\n",
      "        [7.4538e+61],\n",
      "        [3.6185e+61],\n",
      "        [5.0872e+61],\n",
      "        [3.2292e+61]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([2.1233e+61], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 7.698308304101175e+137\n",
      "Weights: tensor([[-9.0527e+71],\n",
      "        [-3.9824e+68],\n",
      "        [-2.1056e+68],\n",
      "        [-2.8148e+68],\n",
      "        [-1.7433e+68]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-1.1650e+68], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 2.2143889865052205e+151\n",
      "Weights: tensor([[4.8922e+78],\n",
      "        [2.1379e+75],\n",
      "        [1.1329e+75],\n",
      "        [1.4062e+75],\n",
      "        [9.3396e+74]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([6.2617e+74], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 4.6053894855867455e+164\n",
      "Weights: tensor([[-1.8827e+85],\n",
      "        [-1.0250e+82],\n",
      "        [-5.6877e+81],\n",
      "        [-6.7307e+81],\n",
      "        [-4.1785e+81]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-2.9495e+81], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 7.090006615746939e+177\n",
      "Weights: tensor([[7.5316e+91],\n",
      "        [3.8970e+88],\n",
      "        [2.1194e+88],\n",
      "        [2.4990e+88],\n",
      "        [1.6515e+88]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([1.1568e+88], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 1.1967159548270788e+191\n",
      "Weights: tensor([[-3.1778e+98],\n",
      "        [-1.5435e+95],\n",
      "        [-8.0980e+94],\n",
      "        [-9.6167e+94],\n",
      "        [-6.7197e+94]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-4.7408e+94], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 2.1736471640794673e+204\n",
      "Weights: tensor([[1.3680e+105],\n",
      "        [6.5024e+101],\n",
      "        [3.4842e+101],\n",
      "        [4.1250e+101],\n",
      "        [2.7577e+101]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([2.0340e+101], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 4.183360808823872e+217\n",
      "Weights: tensor([[-6.1160e+111],\n",
      "        [-2.8402e+108],\n",
      "        [-1.5272e+108],\n",
      "        [-1.8031e+108],\n",
      "        [-1.4279e+108]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-8.8927e+107], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 8.488335816817942e+230\n",
      "Weights: tensor([[2.7758e+118],\n",
      "        [1.2830e+115],\n",
      "        [7.2610e+114],\n",
      "        [8.1273e+114],\n",
      "        [6.4500e+114]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([4.0088e+114], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 1.719306306047891e+244\n",
      "Weights: tensor([[-1.2388e+125],\n",
      "        [-5.5897e+121],\n",
      "        [-3.2621e+121],\n",
      "        [-3.9884e+121],\n",
      "        [-2.7441e+121]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-1.8027e+121], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 3.4744287129474443e+257\n",
      "Weights: tensor([[5.6094e+131],\n",
      "        [2.5086e+128],\n",
      "        [1.3552e+128],\n",
      "        [1.4687e+128],\n",
      "        [1.1240e+128]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([8.0918e+127], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 7.785172824032765e+270\n",
      "Weights: tensor([[-2.7758e+138],\n",
      "        [-1.2815e+135],\n",
      "        [-6.5122e+134],\n",
      "        [-7.0263e+134],\n",
      "        [-5.4655e+134]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-3.8519e+134], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 1.8641485200534564e+284\n",
      "Weights: tensor([[1.3432e+145],\n",
      "        [6.4173e+141],\n",
      "        [3.1715e+141],\n",
      "        [3.4259e+141],\n",
      "        [2.6790e+141]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([1.8806e+141], dtype=torch.float64, requires_grad=True)\n",
      "Loss: 4.700430950868339e+297\n",
      "Weights: tensor([[-6.9991e+151],\n",
      "        [-3.2324e+148],\n",
      "        [-1.6774e+148],\n",
      "        [-1.9433e+148],\n",
      "        [-1.3811e+148]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-9.5236e+147], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[3.6318e+158],\n",
      "        [1.6811e+155],\n",
      "        [8.2292e+154],\n",
      "        [1.1585e+155],\n",
      "        [7.1751e+154]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([4.9519e+154], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[-1.7357e+165],\n",
      "        [-8.5552e+161],\n",
      "        [-4.3322e+161],\n",
      "        [-6.0737e+161],\n",
      "        [-3.6081e+161]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-2.4544e+161], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[8.5508e+171],\n",
      "        [4.1615e+168],\n",
      "        [2.2225e+168],\n",
      "        [2.9513e+168],\n",
      "        [1.7486e+168]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([1.1973e+168], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[-3.9504e+178],\n",
      "        [-2.1168e+175],\n",
      "        [-1.0029e+175],\n",
      "        [-1.4204e+175],\n",
      "        [-7.6945e+174]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-5.7308e+174], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[1.8496e+185],\n",
      "        [9.5757e+181],\n",
      "        [4.6695e+181],\n",
      "        [7.1516e+181],\n",
      "        [3.5911e+181]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([2.6657e+181], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[-9.0298e+191],\n",
      "        [-4.5657e+188],\n",
      "        [-2.2412e+188],\n",
      "        [-3.3197e+188],\n",
      "        [-1.8198e+188]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-1.2755e+188], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[4.4285e+198],\n",
      "        [2.3136e+195],\n",
      "        [1.1765e+195],\n",
      "        [1.8656e+195],\n",
      "        [9.7081e+194]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([6.2384e+194], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[-2.0514e+205],\n",
      "        [-1.0384e+202],\n",
      "        [-5.3316e+201],\n",
      "        [-9.5082e+201],\n",
      "        [-4.5884e+201]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-2.9731e+201], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[9.6206e+211],\n",
      "        [4.8510e+208],\n",
      "        [2.4903e+208],\n",
      "        [4.6711e+208],\n",
      "        [2.1357e+208]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([1.3875e+208], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[-4.3870e+218],\n",
      "        [-2.2483e+215],\n",
      "        [-1.0871e+215],\n",
      "        [-2.0290e+215],\n",
      "        [-8.5779e+214]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-6.4181e+214], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[1.9630e+225],\n",
      "        [1.0168e+222],\n",
      "        [5.2080e+221],\n",
      "        [9.1402e+221],\n",
      "        [3.2974e+221]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([2.8986e+221], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[-9.1403e+231],\n",
      "        [-4.5482e+228],\n",
      "        [-2.3932e+228],\n",
      "        [-4.4070e+228],\n",
      "        [-1.6340e+228]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-1.3284e+228], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[4.2604e+238],\n",
      "        [2.1737e+235],\n",
      "        [1.0606e+235],\n",
      "        [2.1625e+235],\n",
      "        [8.1606e+234]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([6.1891e+234], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[-1.9398e+245],\n",
      "        [-9.6973e+241],\n",
      "        [-5.1633e+241],\n",
      "        [-1.0519e+242],\n",
      "        [-3.7680e+241]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-2.8490e+241], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[8.5804e+251],\n",
      "        [4.3572e+248],\n",
      "        [2.3122e+248],\n",
      "        [4.5956e+248],\n",
      "        [1.4441e+248]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([1.2778e+248], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[-3.6239e+258],\n",
      "        [-1.8891e+255],\n",
      "        [-9.9730e+254],\n",
      "        [-2.0461e+255],\n",
      "        [-6.2602e+254]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-5.5249e+254], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[1.3948e+265],\n",
      "        [7.3916e+261],\n",
      "        [3.7989e+261],\n",
      "        [8.2936e+261],\n",
      "        [2.7092e+261]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([2.2464e+261], dtype=torch.float64, requires_grad=True)\n",
      "Loss: inf\n",
      "Weights: tensor([[-5.1245e+271],\n",
      "        [-2.7739e+268],\n",
      "        [-1.4985e+268],\n",
      "        [-2.9774e+268],\n",
      "        [-9.3538e+267]], dtype=torch.float64, requires_grad=True)\n",
      "Bias: tensor([-8.4095e+267], dtype=torch.float64, requires_grad=True)\n",
      "Trained Weight: tensor([[-5.1245e+271],\n",
      "        [-2.7739e+268],\n",
      "        [-1.4985e+268],\n",
      "        [-2.9774e+268],\n",
      "        [-9.3538e+267]], dtype=torch.float64, requires_grad=True)\n",
      "Trained Bias: tensor([-8.4095e+267], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# training loop without the fancy Trainer class\n",
    "real_data = DataModule(10, x_train, x_val, y_train, y_val)\n",
    "real_model = Module(5, 0.1)\n",
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    for batch in real_data.train_dataloader():\n",
    "        print(f\"Loss: {real_model.training_step(batch).mean()}\")\n",
    "        real_model.update_parameters(batch)\n",
    "        print(f\"Weights: {real_model.w}\")\n",
    "        print(f\"Bias: {real_model.b}\")\n",
    "\n",
    "print(f\"Trained Weight: {real_model.w}\")\n",
    "print(f\"Trained Bias: {real_model.b}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
