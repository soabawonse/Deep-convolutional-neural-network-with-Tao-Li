{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "ROOT = \".data\"\n",
    "VALID_RATIO = 0.9\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.05\n",
    "OUTPUT_CLASSES = 10\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "\n",
    "    def __init__(self, valid_ratio, batch_size):\n",
    "        \n",
    "        \"\"\"Downloads, splits and normalizes the data. Provides dataloaders for training\"\"\"\n",
    "\n",
    "        train_data = datasets.CIFAR10(root=\"../data\",\n",
    "                            train=True,\n",
    "                            download=True)\n",
    "\n",
    "        train_data.data = torch.tensor(train_data.data)\n",
    "\n",
    "        channels = train_data.data.split(1, dim=-1)\n",
    "        channel_tensors = [channel.squeeze(-1) for channel in channels]\n",
    "\n",
    "        means = [z.float().mean() / 255 for z in channel_tensors]\n",
    "        stds = [z.float().std() / 255 for z in channel_tensors]\n",
    "\n",
    "        train_transforms = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=means, std=stds)\n",
    "                                      ])\n",
    "\n",
    "        test_transforms = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=means, std=stds)\n",
    "                                            ])\n",
    "\n",
    "        train_data = datasets.CIFAR10(root=\"../data\",\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=train_transforms)\n",
    "\n",
    "        test_data = datasets.CIFAR10(root=\"../data\",\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=test_transforms)\n",
    "        \n",
    "        n_train_examples = int(len(train_data) * valid_ratio)\n",
    "        n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "        train_data, valid_data = data.random_split(train_data,\n",
    "                                                [n_train_examples, n_valid_examples])\n",
    "\n",
    "        valid_data = copy.deepcopy(valid_data)\n",
    "        valid_data.dataset.transform = test_transforms\n",
    "\n",
    "        self.train_loader = data.DataLoader(train_data,\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=batch_size, num_workers=2)\n",
    "\n",
    "        self.valid_loader = data.DataLoader(valid_data,\n",
    "                                        batch_size=batch_size, num_workers=2)\n",
    "        \n",
    "        self.test_loader = data.DataLoader(test_data, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "\n",
    "    #     # downloads the data\n",
    "    #     self.train_data = datasets.CIFAR10(root=ROOT, train=True, download=True)\n",
    "    #     self.test_data = datasets.CIFAR10(root=ROOT, train=False,download=True)\n",
    "\n",
    "    #     # splitting the data\n",
    "    #     n_train_examples = int(len(self.train_data) * valid_ratio)\n",
    "    #     n_valid_examples = len(self.train_data) - n_train_examples\n",
    "    #     self.train_data, self.valid_data = data.random_split(self.train_data, [n_train_examples, n_valid_examples])\n",
    "    #     self.valid_data = copy.deepcopy(self.valid_data)\n",
    "\n",
    "    #     # normalizing the data\n",
    "\n",
    "    #     t_mean, t_std = self.calculate_mean_std(self.train_data)\n",
    "    #     train_transforms = transforms.Compose([\n",
    "    #                         transforms.ToTensor(),\n",
    "    #                         transforms.Normalize(mean=t_mean, std=t_std)])\n",
    "    #     v_mean, v_std = self.calculate_mean_std(self.valid_data)\n",
    "    #     valid_transforms = transforms.Compose([\n",
    "    #                         transforms.ToTensor(),\n",
    "    #                         transforms.Normalize(mean=v_mean, std=v_std)])\n",
    "    #     te_mean, te_std = self.calculate_mean_std(self.test_data)\n",
    "    #     test_transforms = transforms.Compose([\n",
    "    #                         transforms.ToTensor(),\n",
    "    #                         transforms.Normalize(mean=te_mean, std=te_std)])\n",
    "        \n",
    "    #     self.train_data.transform = train_transforms\n",
    "    #     self.valid_data.transform = valid_transforms\n",
    "    #     self.test_data.transform = test_transforms\n",
    "\n",
    "    #     self.train_loader = data.DataLoader(self.train_data,\n",
    "    #                              shuffle=True,\n",
    "    #                              batch_size=batch_size, num_workers=2)\n",
    "    #     self.valid_loader = data.DataLoader(self.valid_data,\n",
    "    #                              batch_size=batch_size, num_workers=2)\n",
    "        \n",
    "    \n",
    "    # def calculate_mean_std(self, dataset):\n",
    "    #     \"\"\"Calculates the mean and std of a dataset\"\"\"\n",
    "    #     dataset.data = torch.tensor(dataset.data)\n",
    "    #     channels = dataset.data.split(1, dim=-1)\n",
    "    #     channel_tensors = [channel.squeeze(-1) for channel in channels]\n",
    "    #     means = [z.float().mean() / 255 for z in channel_tensors]\n",
    "    #     stds = [z.float().std() / 255 for z in channel_tensors]\n",
    "    #     return means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1, dp=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.dp(F.relu(self.bn1(self.conv1(X))))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return self.dp(F.relu(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, arch, lr=0.1, num_classes=10, dp=0.1):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.dp = dp\n",
    "        self.lr = lr\n",
    "\n",
    "        self.net = nn.Sequential(self.b1())\n",
    "        for i, b in enumerate(arch):\n",
    "            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
    "        self.net.add_module('last', nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.LazyLinear(num_classes)))\n",
    "\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.Dropout(self.dp),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    def block(self, num_residuals, num_channels, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual(num_channels, use_1x1conv=True, strides=2, dp=self.dp))\n",
    "            else:\n",
    "                blk.append(Residual(num_channels, dp=self.dp))\n",
    "        return nn.Sequential(*blk)\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    \n",
    "class ResNet18(ResNet):\n",
    "    def __init__(self, lr=0.1, num_classes=10, dp=0.1):\n",
    "        self.dp = dp\n",
    "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)), lr, num_classes, dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, data, optimizer, criterion, device):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "    def calculate_accuracy(self, y_pred, y):\n",
    "        top_pred = y_pred.argmax(1, keepdim=True)\n",
    "        correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "        acc = correct.float() / y.shape[0]\n",
    "        return acc\n",
    "    \n",
    "    def count_parameters(self, model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        iterator = self.data.train_loader\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            y_pred = self.model(x)\n",
    "\n",
    "            loss = self.criterion(y_pred, y)\n",
    "\n",
    "            acc = self.calculate_accuracy(y_pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        iterator = self.data.valid_loader\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for (x, y) in tqdm(self.iterator, desc=\"Evaluating\", leave=False):\n",
    "\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                y_pred = self.model(x)\n",
    "\n",
    "                loss = self.criterion(y_pred, y)\n",
    "\n",
    "                acc = self.calculate_accuracy(y_pred, y)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(module: nn.Module):\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "data = Data(VALID_RATIO, BATCH_SIZE)\n",
    "model = ResNet18(lr=LEARNING_RATE, num_classes=OUTPUT_CLASSES, dp=DROPOUT)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "model(next(iter(data.train_loader))[0].to(device))\n",
    "model.net.apply(init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, data, optimizer, criterion, device)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Epochs\"):\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc = trainer.train()\n",
    "    valid_loss, valid_acc = trainer.evaluate()\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'IMAGENETMODEL.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = trainer.epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
