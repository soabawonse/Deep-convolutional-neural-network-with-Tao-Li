{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises 7.1.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Do you think that convolutional layers might also be applicable for text data? Which problems might you encounter with language?\n",
    "\n",
    "Response: Likely not, as text is a datatype that more strongly values contextual information. Also, it could be difficult encode text in a format that is applicable to CNNs trained aggnition (images are incoded as pixels with values in black and white as a channel system in color)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What happens with convolutions when an object is at the boundary of an image.\n",
    "\n",
    "Response: Nothing. Because of the translation invarience defined as part of the convolution, where the image is in the image should not affect the convolutions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prove that the convolution is symmetric, i.e., $f * g = g * f$\n",
    "\n",
    "$\\int f(z) g(x - z) dz = \\int g(x) f(x-z) dz$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let t = x - z, z = x - t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\int f(x-t)g(t) dt = \\int g(z)f(x-z) dz$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\int g(z) f(x - z) dz = \\int g(z) f(x - z) dz$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Prove the convolution theorem, i.e., $f * g = \\mathcal{F}^{-1} [\\mathcal{F}[f] \\cdot \\mathcal{F}[g]]$\n",
    "\n",
    "Response: We have not done Fourier Transforms :/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises 7.1. \n",
    "\n",
    "1.) \n",
    "3.) 2dn+1 wwhere n is thea 2n+1 matsquare matrix would be required for the nth sderivative , as yo u would need to be able to walk n steps from the center of each ker to c to compute this .2.) This would look like the third derivative operator. Therefore it would be a 7x7 matrix and would respond to = ...43.) a matrix that computes the weighted average would effectivly blur the image2.8\n",
    "\n",
    "#### 2. Design some kernels manually\n",
    "\n",
    "a. \n",
    "\n",
    "b. \n",
    "\n",
    "c. Have a a kernel whose values sum up to 1, with higher values in the center. For each pixel, the kernel would 'bleed' a tiny bit of the surrounding colors into the current pixel, hence the blurring effect. \n",
    "\n",
    "d. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises 7.3.4\n",
    "\n",
    "#### 6. \n",
    "\n",
    "Response: A stride of $1/2$ would consist of taking some of the same pixel values multiple times. For example for a 1 by 1 convolution for a 3 by 3 image would consist of first convolution on the first, pixel, and then a pixel composed of the average of the first and second pixel, and then the second pixel, and so on. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 7.4.5\n",
    "\n",
    "#### 2. \n",
    "\n",
    "1. The computational cost of forward propogation is $\\underbrace{c_0 \\cdot c_i}_{\\text{Number of times we compute the kernels}} \\cdot \\underbrace{k_h \\cdot k_w}_{\\text{Operations per kernel}} \\cdot \\underbrace{\\lfloor(n - k_h + p_h + s_h)/s_h \\rfloor \\cdot \\lfloor(n_w - k_w + p_w + s_w)/s_w \\rfloor}_{\\text{Number of output pixels per kernel}}$\n",
    "\n",
    "2. The memory footprint is $\\underbrace{c_0 \\cdot c_i}_\\text{Number of kernels} \\cdot \\underbrace{k_h \\cdot k_w}_{\\text{Memory per kernel}}$\n",
    "\n",
    "#### 3. \n",
    "\n",
    "Doubling input channels and doubling output channels leads to four times the number of calculations, and doubling the padding doubles the number of calculations. \n",
    "\n",
    "#### 5.\n",
    "\n",
    "Flatten the matrix and rearranging the weights and inputs, and then reshaping them back into the needed output dimensions after the matrix multiplication. \n",
    "\n",
    "\n",
    "#### 6."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 7.5.5\n",
    "\n",
    "#### 3. \n",
    "\n",
    "1. \n",
    "\n",
    "$\\frac{ReLu(a-b) + b + ReLu(b-a) + a}{2}$\n",
    "\n",
    "2. \n",
    "\n",
    "\n",
    "\n",
    "#### 4. \n",
    "\n",
    "$\\frac{c \\times h \\times w \\times p_h \\times p_w}{s_h \\times s_w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4.5 Exercises : 2, 3, 5, 6\n",
    "# 7.5.5 Exercises : 3,4 \n",
    "# 7.6.4 Exercises: 1,2 3, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(module):\n",
    "    \"\"\"Initialize weights for the CNN\"\"\"\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "# EX1\n",
    "class LeNet2(d2l.Classifier):\n",
    "    \"\"\"The LeNet-5 model\"\"\"\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.ReLU()\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.ReLU(),\n",
    "            nn.LazyLinear(84), nn.ReLU(),\n",
    "            nn.LazyLinear(num_classes))\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model2 = LeNet2(lr=0.1)\n",
    "model2.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\n",
    "trainer.fit(model2, data)\n",
    "\n",
    "ahjdkasdkasd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX2\n",
    "class LeNet2(d2l.Classifier):\n",
    "    \"\"\"The LeNet-5 model\"\"\"\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.net.parameters(),self.lr)\n",
    "\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(16, kernel_size=11, padding=5), nn.ReLU()\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(24, kernel_size=7), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(32, kernel_size=5), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.ReLU(),\n",
    "            nn.LazyLinear(84), nn.ReLU(),\n",
    "            nn.LazyLinear(42), nn.ReLU(),\n",
    "            nn.LazyLinear(num_classes))\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=15, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model2 = LeNet2(lr=0.01)\n",
    "model2.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\n",
    "trainer.fit(model2, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
